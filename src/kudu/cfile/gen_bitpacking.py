#!/usr/bin/env python

import os
import sys

BITS = range(1, 33)

# TODO(todd) should we enforce aligned input/output for these?
# may need the format to ensure alignment.

def gen_packwithoutmask(bitsize, num_ints=128):
    print("""
template<typename INT> // either uint32_t or uint64_t
__attribute__((noinline))
void simd_fastpackwithoutmask#BITSIZE#(const INT* __restrict__ in,
    __m128i* __restrict__ out) {
  __m128i out_reg;
  __m128i in_reg;

  #define LOAD load_4x32
  #define STORE _mm_storeu_si128
  #define OR _mm_or_si128
  #define SHIFT_LEFT _mm_slli_epi32
  #define SHIFT_RIGHT _mm_srli_epi32
""".replace("#BITSIZE#", str(bitsize)))
    shift = 0
    for i in xrange(num_ints/4):
        print("  in_reg = LOAD(in);")
        print("  in += 4;")
        if shift == 0:
            print("  out_reg = in_reg;")
        else:
            print("  out_reg = OR(out_reg, SHIFT_LEFT(in_reg, {shift}));".format(
             shift=shift))
        shift += bitsize
        if shift >= 32:
            print("  STORE(out++, out_reg);")
            shift -= 32
            if shift:
                print("  // {rem} most significant bits of in_reg fell off the left in above shift."
                      .format(rem=shift))
                print("  // Shift right to recover them in next output word")
                print("  out_reg = SHIFT_RIGHT(in_reg, {bits});".format(bits=bitsize - shift))
        print
    assert shift == 0
    print("#undef LOAD")
    print("#undef STORE")
    print("#undef OR")
    print("#undef SHIFT_LEFT")
    print("#undef SHIFT_RIGHT")
    print "}"


def gen_unpack(bitsize, num_ints=128):
    print("""
template<typename INT>
__attribute__((noinline))
void simd_fastunpack#BITSIZE#(const __m128i* __restrict__ in,
    INT* __restrict__ out) {
  __m128i out_reg;
  __m128i in_reg = _mm_loadu_si128(in++);
  const __m128i mask = _mm_set1_epi32((1ULL << #BITSIZE#) - 1);
""".replace("#BITSIZE#", str(bitsize)))
    shift = 0
    for i in xrange(num_ints/4):
        if shift != 0:
          print("  out_reg = _mm_srli_epi32(in_reg, {shift});"
                .format(shift=shift))
        else:
          print("  out_reg = in_reg;")
        shift += bitsize
        if shift < 32:
            # unless the bitpacked value extended all the way to the
            # MSB of the source uint32, perform masking.
            print("  out_reg = _mm_and_si128(out_reg, mask);")
        else:
            # Done with this input byte.
            print("  in_reg = _mm_loadu_si128(in++);")
            shift -= 32
            if shift:
                print("  // {bits} bits still need to be recovered...".format(bits=shift))
                print("  out_reg = _mm_or_si128(out_reg, _mm_and_si128(_mm_slli_epi32(in_reg, {bits}), mask));"
                      .format(bits=bitsize - shift))

        print("  store_4x32(out_reg, out);")
        print("  out += 4;")
        print
    assert shift == 0
    print "}"

def gen_wrapper(method, in_type, out_type):
    print("""
void %(method)s(
    const %(in_type)s* __restrict__ in,
    %(out_type)s* __restrict__ out,
    int num_bits) {
  DCHECK_GE(num_bits, 0);
  DCHECK_LE(num_bits, 32);
  switch (num_bits) {
""" % dict(method=method, in_type=in_type, out_type=out_type))

    # For unpacking, if the values are all zero, we need to
    # actually unpack the 0s. For packing, there will be no
    # output so we can just return if num_bits = 0.
    if 'unpack' in method:
        print("""
    case 0:
      memset(out, 0, sizeof(out[0]) * 128);
      break;
""")
    for i in BITS:
        print("    case {i}: {method}{i}(in, out); return;"
              .format(i=i, method=method))
    print("  }")
    print("}")

# redirect stdout
def main(argv):
    if len(argv) == 2:
        sys.stdout = open(sys.argv[1], "w")

    print("// AUTO-GENERATED BY {}".format(os.path.realpath(argv[0])))
    print("""
#include <stdint.h>
#include <xmmintrin.h>
#include <immintrin.h>
#include <glog/logging.h>

namespace kudu {
namespace cfile {
namespace bp {
namespace {

template<class SrcInt>
__m128i load_4x32(const SrcInt * __restrict__ src) {
  uint32_t x[4] = {
    static_cast<SrcInt>(src[0]),
    static_cast<SrcInt>(src[1]),
    static_cast<SrcInt>(src[2]),
    static_cast<SrcInt>(src[3])};
   return _mm_loadu_si128((const __m128i*)x);
}

template<>
__m128i load_4x32(const uint64_t * __restrict__ src) {
   return _mm_setr_epi32(src[0], src[1], src[2], src[3]);
}

// Store 4 32-bit integers from SSE 4 n-bit integers.
template<typename DstInt>
void store_4x32(__m128i m, DstInt * __restrict__ dst) {
  // This gets optimized into a sequence of instructions that
  // more efficient than the four separate extracts written here.
  *dst++ = static_cast<DstInt>(static_cast<uint32_t>(_mm_extract_epi32(m, 0)));
  *dst++ = static_cast<DstInt>(static_cast<uint32_t>(_mm_extract_epi32(m, 1)));
  *dst++ = static_cast<DstInt>(static_cast<uint32_t>(_mm_extract_epi32(m, 2)));
  *dst++ = static_cast<DstInt>(static_cast<uint32_t>(_mm_extract_epi32(m, 3)));
}

// Specialization for storing 32-bit.
template<>
void store_4x32(__m128i m, uint32_t * __restrict__ dst) {
   _mm_storeu_si128(reinterpret_cast<__m128i*>(dst), m);
}


""")
    for i in BITS:
        gen_unpack(i)
    for i in BITS:
        gen_packwithoutmask(i)

    print("} // anonymous namespace")

    for scalar_type_bits in [8, 16, 32, 64]:
        scalar_type = "uint{}_t".format(scalar_type_bits)
        gen_wrapper("simd_fastunpack", in_type="__m128i", out_type=scalar_type)
        gen_wrapper("simd_fastpackwithoutmask", in_type=scalar_type, out_type="__m128i")
    print("} // anonymous bp")
    print("} // anonymous cfile")
    print("} // anonymous kudu")

if __name__ == "__main__":
    main(sys.argv)
